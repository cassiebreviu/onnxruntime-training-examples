{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Accelerate finetuning of GPT2 model for Language Modeling task using ONNX Runtime Training\r\n",
    "This notebook contains a walkthrough of using ONNX Runtime Training in Azure Machine Learning service to finetune [GPT2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) models. This example uses ONNX Runtime Training to fine-tune the GPT2 PyTorch model maintained at https://github.com/huggingface/transformers.\r\n",
    "Specificaly, we showcase finetuning the [pretrained GPT2-medium](https://huggingface.co/transformers/pretrained_models.html), which has 345M parameters using ORT.\r\n",
    "\r\n",
    "Steps:\r\n",
    "- Intialize an AzureML workspace\r\n",
    "- Register a datastore to use preprocessed data for training\r\n",
    "- Create an AzureML experiment\r\n",
    "- Provision a compute target\r\n",
    "- Create a PyTorch Estimator\r\n",
    "- Configure and Run\r\n",
    "\r\n",
    "Prerequisites\r\n",
    "If you are using an Azure Machine Learning [Compute Instance](https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-instance) you are all set. Otherwise, you need to setup your environment by installing AzureML Python SDK to run this notebook. Refer to [How to use Estimator in Azure ML](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training-with-deep-learning/how-to-use-estimator/how-to-use-estimator.ipynb) notebook first if you haven't already to establish your connection to the AzureML Workspace. \r\n",
    "\r\n",
    "Refer to instructions at https://github.com/microsoft/onnxruntime-training-examples/blob/master/huggingface-gpt2/README.md before running the steps below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check SDK installation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import requests\r\n",
    "import sys\r\n",
    "import re\r\n",
    "\r\n",
    "# AzureML libraries\r\n",
    "import azureml.core\r\n",
    "from azureml.core import Experiment, Workspace, Datastore, Run\r\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
    "from azureml.core.compute_target import ComputeTargetException\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "from azureml.core.container_registry import ContainerRegistry\r\n",
    "from azureml.core.runconfig import MpiConfiguration, RunConfiguration, DEFAULT_GPU_IMAGE\r\n",
    "from azureml.train.dnn import PyTorch\r\n",
    "from azureml.train.estimator import Estimator\r\n",
    "from azureml.widgets import RunDetails\r\n",
    "\r\n",
    "from azure.common.client_factory import get_client_from_cli_profile\r\n",
    "from azure.mgmt.containerregistry import ContainerRegistryManagementClient\r\n",
    "\r\n",
    "# Check core SDK version number\r\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### AzureML Workspace setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "# Create or retrieve Azure machine learning workspace\r\n",
    "# see https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py\r\n",
    "ws = Workspace.get(name=\"demo\", subscription_id='47c81f7b-f720-4f17-9116-69d540091679', resource_group='demo')\r\n",
    "\r\n",
    "# Print workspace attributes\r\n",
    "print('Workspace name: ' + ws.name, \r\n",
    "      'Workspace region: ' + ws.location, \r\n",
    "      'Subscription id: ' + ws.subscription_id, \r\n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Workspace name: demo\n",
      "Workspace region: westus2\n",
      "Subscription id: 47c81f7b-f720-4f17-9116-69d540091679\n",
      "Resource group: demo\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Register Datastore\r\n",
    "Before running the step below, data prepared using the instructions at https://github.com/microsoft/onnxruntime-training-examples/blob/master/huggingface-gpt2/README.md should be transferred to an Azure Blob container referenced in the `Datastore` registration step. Refer to the documentation at https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data for details on using data in Azure ML experiments."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a datastore from blob storage containing training data.\r\n",
    "# Consult README.md for instructions downloading and uploading training data.\r\n",
    "#ds = Datastore.register_azure_blob_container(workspace=ws, \r\n",
    "#                                             datastore_name='wikitext',\r\n",
    "#                                             account_name='demo1879244313', \r\n",
    "#                                             account_key='',\r\n",
    "#                                             container_name='tokenfiles')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds = Datastore.get(workspace=ws, datastore_name='gpt_wikitext')\r\n",
    "# Print datastore attributes\r\n",
    "print('Datastore name: ' + ds.name, \r\n",
    "      'Container name: ' + ds.container_name, \r\n",
    "      'Datastore type: ' + ds.datastore_type, \r\n",
    "      'Workspace name: ' + ds.workspace.name, sep = '\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Dataset.get_all(workspace=ws)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import azureml.core\r\n",
    "from azureml.core import Workspace, Datastore, Dataset\r\n",
    "\r\n",
    "train_data = Dataset.get_by_name(name='wikitext_train', workspace=ws)\r\n",
    "valid_data = Dataset.get_by_name(name='wikitext_valid', workspace=ws)\r\n",
    "\r\n",
    "print(train_data.name)\r\n",
    "print(valid_data.name)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create AzureML Compute Cluster\n",
    "This recipe is supported on Azure Machine Learning Service using 16 x Standard_NC24rs_v3 or 8 x Standard_ND40rs_v2 VMs. In the next step, you will create an AzureML Compute cluster of Standard_NC40s_v2 GPU VMs with the specified name, if it doesn't already exist in your workspace. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create GPU cluster\r\n",
    "#gpu_cluster_name = \"ortgptfinetune\" \r\n",
    "gpu_cluster_name = \"cassieb1\" \r\n",
    "try:\r\n",
    "    gpu_compute_target = ComputeTarget(workspace=ws, name=gpu_cluster_name)\r\n",
    "    print('Found existing compute target.')\r\n",
    "except ComputeTargetException:\r\n",
    "    print('Creating a new compute target...')\r\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_ND40rs_v2', min_nodes=0, max_nodes=8)\r\n",
    "    gpu_compute_target = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\r\n",
    "    gpu_compute_target.wait_for_completion(show_output=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Estimator\n",
    "Notes before running the following step:\n",
    "* Update the following step to replace two occurences of `<blob-path-to-training-data>` with the actual path in the datastore to the training data.\n",
    "* If you followed instructions at https://github.com/microsoft/onnxruntime-training-examples/blob/master/huggingface-gpt2/README.md to prepare data, make sure that the data and others files that are not code or config are moved out `workspace` directory. Data files should have been moved to a `Datastore` to use in training. \n",
    "* Update the occurance of `<tagged-onnxruntime-gpt-container>` with the tag of the built docker image pushed to a container registry. Similarly, update the `<azure-subscription-id>` and `<container-registry-resource-group>` with the contair registry's subscription ID and resource group.\n",
    "\n",
    "\n",
    "| VM SKU             | GPU memory   | gpu_count |    ORT_batch_size    |\n",
    "| ------------------ |:----------------:|:---------:|:-------:|\n",
    "| Standard_ND40rs_v2 | 32 GB            | 8         | 4   |\n",
    "| Standard_NC24rs_v3 | 16 GB            | 4         | 1   |\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this directory should contain run_language_modeling.py, after files copied over based on the instructions at https://github.com/microsoft/onnxruntime-training-examples/blob/master/huggingface-gpt2/README.md \r\n",
    "project_folder = 'orttrainer/huggingface-gpt2/transformers/examples'\r\n",
    "\r\n",
    "# set MPI configuration\r\n",
    "# set processes per node to be equal to GPU count on SKU.\r\n",
    "# this will change based on NC v/s ND series VMs\r\n",
    "mpi_distr_config = MpiConfiguration(process_count_per_node=8, node_count=4)\r\n",
    "\r\n",
    "experiment = Experiment(ws,'onnxruntime-gpt2')\r\n",
    "\r\n",
    "import uuid\r\n",
    "output_id = uuid.uuid1().hex\r\n",
    "\r\n",
    "# Define the script parameters.\r\n",
    "# To run training PyTorch instead of ORT, remove the --ort_trainer flag.\r\n",
    "# To run evaluation using PyTorch instead of ORT, use the --do_eval_in_torch flag.\r\n",
    "script_params = {\r\n",
    "    '--model_type' : 'gpt2-medium', \r\n",
    "    '--model_name_or_path' : 'gpt2-medium', \r\n",
    "    '--tokenizer_name' : 'gpt2-medium', \r\n",
    "    '--config_name' : 'gpt2-medium', \r\n",
    "    '--do_eval' : '', \r\n",
    "    '--do_train': '', \r\n",
    "    '--train_data_file' : ds.path('/home/azureuser/cloudfiles/data/datastore/gpt_data/wiki.train.tokens').as_mount(),\r\n",
    "    '--eval_data_file' : ds.path('/home/azureuser/cloudfiles/data/datastore/gpt_data/wiki.valid.tokens').as_mount(),\r\n",
    "    '--output_dir' : ds.path(f'output/{experiment.name}/{output_id}/').as_mount(), \r\n",
    "    '--per_gpu_train_batch_size' : '4', \r\n",
    "    '--per_gpu_eval_batch_size' : '4', \r\n",
    "    '--gradient_accumulation_steps' : '4',\r\n",
    "    '--block_size' : '1024', \r\n",
    "    '--weight_decay' : '0.01', \r\n",
    "    '--overwrite_output_dir' : '', \r\n",
    "    '--num_train_epocs' : '5',\r\n",
    "    '--ort_trainer' : ''\r\n",
    "    }\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "# List the files in the mounted path\r\n",
    "print(os.listdir(\"/home/azureuser/cloudfiles/data/datastore/gpt_data\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core import Environment\r\n",
    "from azureml.core import ScriptRunConfig\r\n",
    "from azureml.core.runconfig import DockerConfiguration\r\n",
    "\r\n",
    "onnxruntime_gpu_env = Environment(\"onnxruntime_gpu_env\")\r\n",
    "# Specify custom Docker base image and registry, if you don't want to use the defaults\r\n",
    "onnxruntime_gpu_env.docker.base_image=\"onnxruntime-gpt\"\r\n",
    "onnxruntime_gpu_env.docker.base_image_registry=\"demoregazure.azurecr.io\"\r\n",
    "docker_config = DockerConfiguration(use_docker=True)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "script_run_config = ScriptRunConfig(\r\n",
    "                      source_directory=project_folder,\r\n",
    "                      script='run_language_modeling.py',\r\n",
    "                      arguments = script_params,\r\n",
    "                      #compute\r\n",
    "                      compute_target=gpu_compute_target,\r\n",
    "                      # custom docker image\r\n",
    "                      environment=onnxruntime_gpu_env,\r\n",
    "                      #mpi\r\n",
    "                      distributed_job_config=mpi_distr_config,\r\n",
    "                      docker_runtime_config=docker_config\r\n",
    "                      )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run AzureML experiment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "experiment.submit(script_run_config)\r\n",
    "experiment.wait_for_completion(show_output=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Submit ORT run (check logs from Outputs + logs tab of corresponding link)\r\n",
    "#run = experiment.submit(estimator_ort)\r\n",
    "#RunDetails(run).show()\r\n",
    "#print(run.get_portal_url())"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - PyTorch",
   "language": "python",
   "name": "azureml_py38_pytorch"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}