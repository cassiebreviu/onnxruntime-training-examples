{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Accelerate finetuning of GPT2 model for Language Modeling task using ONNX Runtime Training\r\n",
        "This notebook contains a walkthrough of using ONNX Runtime Training in Azure Machine Learning service to finetune [GPT2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) models. This example uses ONNX Runtime Training to fine-tune the GPT2 PyTorch model maintained at https://github.com/huggingface/transformers.\r\n",
        "Specificaly, we showcase finetuning the [pretrained GPT2-medium](https://huggingface.co/transformers/pretrained_models.html), which has 345M parameters using ORT.\r\n",
        "\r\n",
        "Steps:\r\n",
        "- Intialize an AzureML workspace\r\n",
        "- Register a datastore to use preprocessed data for training\r\n",
        "- Create an AzureML experiment\r\n",
        "- Provision a compute target\r\n",
        "- Create a PyTorch Estimator\r\n",
        "- Configure and Run\r\n",
        "\r\n",
        "Prerequisites\r\n",
        "If you are using an Azure Machine Learning [Compute Instance](https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-instance) you are all set. Otherwise, you need to setup your environment by installing AzureML Python SDK to run this notebook. Refer to [How to use Estimator in Azure ML](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training-with-deep-learning/how-to-use-estimator/how-to-use-estimator.ipynb) notebook first if you haven't already to establish your connection to the AzureML Workspace. \r\n",
        "\r\n",
        "Refer to instructions at https://github.com/microsoft/onnxruntime-training-examples/blob/master/huggingface-gpt2/README.md before running the steps below."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check SDK installation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import os\r\n",
        "import requests\r\n",
        "import sys\r\n",
        "import re\r\n",
        "\r\n",
        "# AzureML libraries\r\n",
        "import azureml.core\r\n",
        "from azureml.core import Experiment, Workspace, Datastore, Run\r\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "from azureml.core.conda_dependencies import CondaDependencies\r\n",
        "from azureml.core.container_registry import ContainerRegistry\r\n",
        "from azureml.core.runconfig import MpiConfiguration, RunConfiguration, DEFAULT_GPU_IMAGE\r\n",
        "from azureml.train.dnn import PyTorch\r\n",
        "from azureml.train.estimator import Estimator\r\n",
        "from azureml.widgets import RunDetails\r\n",
        "\r\n",
        "from azure.common.client_factory import get_client_from_cli_profile\r\n",
        "from azure.mgmt.containerregistry import ContainerRegistryManagementClient\r\n",
        "\r\n",
        "# Check core SDK version number\r\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SDK version: 1.34.0\n"
          ]
        }
      ],
      "metadata": {
        "gather": {
          "logged": 1632154010440
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AzureML Workspace setup"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# Create or retrieve Azure machine learning workspace\r\n",
        "# see https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py\r\n",
        "ws = Workspace.get(name=\"demo\", subscription_id='', resource_group='demo')\r\n",
        "\r\n",
        "# Print workspace attributes\r\n",
        "print('Workspace name: ' + ws.name, \r\n",
        "      'Workspace region: ' + ws.location, \r\n",
        "      'Subscription id: ' + ws.subscription_id, \r\n",
        "      'Resource group: ' + ws.resource_group, sep = '\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Workspace name: demo\n",
            "Workspace region: westus2\n",
            "Subscription id: 47c81f7b-f720-4f17-9116-69d540091679\n",
            "Resource group: demo\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Register Datastore\r\n",
        "Before running the step below, data prepared using the instructions at https://github.com/microsoft/onnxruntime-training-examples/blob/master/huggingface-gpt2/README.md should be transferred to an Azure Blob container referenced in the `Datastore` registration step. Refer to the documentation at https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data for details on using data in Azure ML experiments."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "# Create a datastore from blob storage containing training data.\r\n",
        "# Consult README.md for instructions downloading and uploading training data.\r\n",
        "#ds = Datastore.register_azure_blob_container(workspace=ws, \r\n",
        "#                                             datastore_name='wikitext',\r\n",
        "#                                             account_name='demo1879244313', \r\n",
        "#                                             account_key='',\r\n",
        "#                                             container_name='tokenfiles')"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1632154012516
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "ds = Datastore.get(workspace=ws, datastore_name='gpt_wikitext')\r\n",
        "# Print datastore attributes\r\n",
        "print('Datastore name: ' + ds.name, \r\n",
        "      'Container name: ' + ds.container_name, \r\n",
        "      'Datastore type: ' + ds.datastore_type, \r\n",
        "      'Workspace name: ' + ds.workspace.name, sep = '\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datastore name: gpt_wikitext\n",
            "Container name: wikitext\n",
            "Datastore type: AzureBlob\n",
            "Workspace name: demo\n"
          ]
        }
      ],
      "metadata": {
        "gather": {
          "logged": 1632154036421
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "import azureml.core\r\n",
        "from azureml.core import Workspace, Datastore, Dataset\r\n",
        "\r\n",
        "train_data = Dataset.get_by_name(name='wikitext_train', workspace=ws)\r\n",
        "valid_data = Dataset.get_by_name(name='wikitext_valid', workspace=ws)\r\n",
        "\r\n",
        "print(train_data.name)\r\n",
        "print(valid_data.name)\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wikitext_train\n",
            "wikitext_valid\n"
          ]
        }
      ],
      "metadata": {
        "gather": {
          "logged": 1632154018749
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create AzureML Compute Cluster\n",
        "This recipe is supported on Azure Machine Learning Service using 16 x Standard_NC24rs_v3 or 8 x Standard_ND40rs_v2 VMs. In the next step, you will create an AzureML Compute cluster of Standard_NC40s_v2 GPU VMs with the specified name, if it doesn't already exist in your workspace. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# Create GPU cluster\r\n",
        "#gpu_cluster_name = \"ortgptfinetune\" \r\n",
        "gpu_cluster_name = \"cassieb1\" \r\n",
        "try:\r\n",
        "    gpu_compute_target = ComputeTarget(workspace=ws, name=gpu_cluster_name)\r\n",
        "    print('Found existing compute target.')\r\n",
        "except ComputeTargetException:\r\n",
        "    print('Creating a new compute target...')\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_ND40rs_v2', min_nodes=0, max_nodes=8)\r\n",
        "    gpu_compute_target = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\r\n",
        "    gpu_compute_target.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing compute target.\n"
          ]
        }
      ],
      "metadata": {
        "gather": {
          "logged": 1632156033470
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Estimator\n",
        "Notes before running the following step:\n",
        "* Update the following step to replace two occurences of `<blob-path-to-training-data>` with the actual path in the datastore to the training data.\n",
        "* If you followed instructions at https://github.com/microsoft/onnxruntime-training-examples/blob/master/huggingface-gpt2/README.md to prepare data, make sure that the data and others files that are not code or config are moved out `workspace` directory. Data files should have been moved to a `Datastore` to use in training. \n",
        "* Update the occurance of `<tagged-onnxruntime-gpt-container>` with the tag of the built docker image pushed to a container registry. Similarly, update the `<azure-subscription-id>` and `<container-registry-resource-group>` with the contair registry's subscription ID and resource group.\n",
        "\n",
        "\n",
        "| VM SKU             | GPU memory   | gpu_count |    ORT_batch_size    |\n",
        "| ------------------ |:----------------:|:---------:|:-------:|\n",
        "| Standard_ND40rs_v2 | 32 GB            | 8         | 4   |\n",
        "| Standard_NC24rs_v3 | 16 GB            | 4         | 1   |\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# this directory should contain run_language_modeling.py, after files copied over based on the instructions at https://github.com/microsoft/onnxruntime-training-examples/blob/master/huggingface-gpt2/README.md \r\n",
        "#project_folder = 'orttrainer/huggingface-gpt2/transformers/examples'\r\n",
        "project_folder = '.'\r\n",
        "\r\n",
        "# set MPI configuration\r\n",
        "# set processes per node to be equal to GPU count on SKU.\r\n",
        "# this will change based on NC v/s ND series VMs\r\n",
        "mpi_distr_config = MpiConfiguration(process_count_per_node=4, node_count=1)\r\n",
        "\r\n",
        "experiment = Experiment(ws,'onnxruntime-gpt2')\r\n",
        "\r\n",
        "import uuid\r\n",
        "output_id = uuid.uuid1().hex\r\n",
        "\r\n",
        "output_dir = f'/output/{experiment.name}/{output_id}/'\r\n",
        "print(output_dir)\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/output/onnxruntime-gpt2/f7174c1c1a4411ecbe6b000d3af6b150/\n"
          ]
        }
      ],
      "metadata": {
        "gather": {
          "logged": 1632157760365
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# Define the script parameters.\r\n",
        "# To run training PyTorch instead of ORT, remove the --ort_trainer flag.\r\n",
        "# To run evaluation using PyTorch instead of ORT, use the --do_eval_in_torch flag.\r\n",
        "script_params = [\r\n",
        "    '--model_type', 'gpt2-medium', \r\n",
        "    '--model_name_or_path', 'gpt2-medium', \r\n",
        "    '--tokenizer_name' , 'gpt2-medium', \r\n",
        "    '--config_name' , 'gpt2-medium', \r\n",
        "    '--do_eval' , '', \r\n",
        "    '--do_train', '', \r\n",
        "    '--train_data_file' ,'/home/azureuser/cloudfiles/data/datastore/gpt_data/wiki.train.tokens',\r\n",
        "    '--eval_data_file' , '/home/azureuser/cloudfiles/data/datastore/gpt_data/wiki.valid.tokens',\r\n",
        "    '--output_dir' , output_dir, \r\n",
        "    '--per_gpu_train_batch_size' , '4', \r\n",
        "    '--per_gpu_eval_batch_size' , '4', \r\n",
        "    '--gradient_accumulation_steps' , '4',\r\n",
        "    '--block_size' , '1024', \r\n",
        "    '--weight_decay' , '0.01', \r\n",
        "    '--overwrite_output_dir' , '', \r\n",
        "    '--num_train_epocs' , '5',\r\n",
        "    '--ort_trainer' , ''\r\n",
        "    ]"
      ],
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632163043569
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "type(script_params)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1632163046555
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "import os\r\n",
        "# List the files in the mounted path\r\n",
        "print(os.listdir(\"/home/azureuser/cloudfiles/data/datastore/gpt_data\"))\r\n",
        "ds"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['UI', 'wiki.test.tokens', 'wiki.train.tokens', 'wiki.valid.tokens']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\n",
              "  \"name\": \"gpt_wikitext\",\n",
              "  \"container_name\": \"wikitext\",\n",
              "  \"account_name\": \"demo1879244313\",\n",
              "  \"protocol\": \"https\",\n",
              "  \"endpoint\": \"core.windows.net\"\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "metadata": {
        "gather": {
          "logged": 1632163048610
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "from azureml.core import Environment\r\n",
        "from azureml.core import ScriptRunConfig\r\n",
        "from azureml.core.runconfig import DockerConfiguration\r\n",
        "\r\n",
        "docker_config = DockerConfiguration(use_docker=True)\r\n",
        "## env created based on my docker image in aml\r\n",
        "onnxruntime_gpu_env = Environment.get(workspace=ws, name=\"onnxruntime-gpt\")\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1632163057459
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "script_run_config = ScriptRunConfig(\r\n",
        "                      source_directory=project_folder,\r\n",
        "                      script='run_language_modeling.py',\r\n",
        "                      arguments = script_params,\r\n",
        "                      #compute\r\n",
        "                      compute_target=gpu_compute_target,\r\n",
        "                      # custom docker image\r\n",
        "                      environment=onnxruntime_gpu_env,\r\n",
        "                      #mpi\r\n",
        "                      distributed_job_config=mpi_distr_config,\r\n",
        "                      docker_runtime_config=docker_config\r\n",
        "                      )"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1632164240856
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run AzureML experiment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "experiment.submit(script_run_config)\r\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ExperimentExecutionException",
          "evalue": "ExperimentExecutionException:\n\tMessage: /mnt/batch/tasks/shared/LS_root/mounts/clusters/cassieb1/code/run_language_modeling.py script path doesn't exist. The script should be inside the project folder\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/cassieb1/code/run_language_modeling.py script path doesn't exist. The script should be inside the project folder\"\n    }\n}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mExperimentExecutionException\u001b[0m              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-0fc49c4dac6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_run_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/azureml/core/experiment.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, config, tags, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0msubmit_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_experiment_submit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"submit config {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/azureml/core/script_run_config.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(script_run_config, workspace, experiment_name, run_id, _parent_run_id, credential_passthrough)\u001b[0m\n\u001b[1;32m     60\u001b[0m     collect_datasets_usage(module_logger, _SCRIPT_RUN_SUBMIT_ACTIVITY, inputs,\n\u001b[1;32m     61\u001b[0m                            workspace, run_config.target)\n\u001b[0;32m---> 62\u001b[0;31m     run = _commands.start_run(project, run_config,\n\u001b[0m\u001b[1;32m     63\u001b[0m                               \u001b[0mtelemetry_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscript_run_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_telemetry_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                               run_id=run_id, parent_run_id=_parent_run_id)\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/azureml/_execution/_commands.py\u001b[0m in \u001b[0;36mstart_run\u001b[0;34m(project_object, run_config_object, run_id, injected_files, telemetry_values, parent_run_id, prepare_only, check)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \"\"\"\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     setup = _setup_run(project_object, run_config_object,\n\u001b[0m\u001b[1;32m     97\u001b[0m                        injected_files=injected_files, prepare_only=prepare_only)\n\u001b[1;32m     98\u001b[0m     \u001b[0mcustom_target_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_target_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/azureml/_execution/_commands.py\u001b[0m in \u001b[0;36m_setup_run\u001b[0;34m(project_object, run_config_object, prepare_only, injected_files)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 os.path.relpath(full_script_path, project_object.project_directory))\n\u001b[1;32m    157\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             raise ExperimentExecutionException(\"{} script path doesn't exist. \"\n\u001b[0m\u001b[1;32m    159\u001b[0m                                                \u001b[0;34m\"The script should be inside the project \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                                                \"folder\".format(full_script_path))\n",
            "\u001b[0;31mExperimentExecutionException\u001b[0m: ExperimentExecutionException:\n\tMessage: /mnt/batch/tasks/shared/LS_root/mounts/clusters/cassieb1/code/run_language_modeling.py script path doesn't exist. The script should be inside the project folder\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/cassieb1/code/run_language_modeling.py script path doesn't exist. The script should be inside the project folder\"\n    }\n}"
          ]
        }
      ],
      "metadata": {
        "gather": {
          "logged": 1632164248533
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "azureml_py38_pytorch",
      "language": "python",
      "display_name": "Python 3.8 - PyTorch"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "azureml_py38_pytorch"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}